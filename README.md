# ðŸ‡§ðŸ‡· CNPJ Data Pipeline

Um script modular e configurÃ¡vel para processar arquivos CNPJ da Receita Federal do Brasil. Processamento inteligente de 50+ milhÃµes de empresas com suporte a mÃºltiplos bancos de dados.

## CaracterÃ­sticas Principais

- **Arquitetura Modular**: SeparaÃ§Ã£o clara de responsabilidades com camada de abstraÃ§Ã£o de banco de dados
- **Multi-Banco**: PostgreSQL totalmente suportado, com placeholders para MySQL, BigQuery e SQLite
- **Processamento Inteligente**: AdaptaÃ§Ã£o automÃ¡tica da estratÃ©gia baseada em recursos disponÃ­veis
- **Downloads Paralelos**: EstratÃ©gia configurÃ¡vel para otimizar velocidade de download
- **Processamento Incremental**: Rastreamento de arquivos processados para evitar duplicaÃ§Ãµes
- **Performance Otimizada**: OperaÃ§Ãµes bulk eficientes com tratamento de conflitos
- **ConfiguraÃ§Ã£o Simples**: Setup interativo + variÃ¡veis de ambiente

## InÃ­cio RÃ¡pido

### OpÃ§Ã£o 1: Setup Interativo (Recomendado)

```bash
# Clone o repositÃ³rio
git clone https://github.com/cnpj-chat/cnpj-data-pipeline
cd cnpj-data-pipeline

# Execute o assistente de configuraÃ§Ã£o
python setup.py
```

O assistente irÃ¡:
- Detectar recursos do sistema
- Configurar conexÃ£o com banco de dados
- Instalar dependÃªncias necessÃ¡rias
- Criar configuraÃ§Ã£o otimizada

### OpÃ§Ã£o 2: ConfiguraÃ§Ã£o Manual

```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar ambiente
cp env.example .env
# Editar .env com suas configuraÃ§Ãµes

# Executar
python main.py
```

### Docker

```bash
# PostgreSQL (padrÃ£o)
docker-compose --profile postgres up --build

# Com configuraÃ§Ãµes customizadas
DATABASE_BACKEND=postgresql BATCH_SIZE=100000 docker-compose --profile postgres up

# Com filtros de dados
docker-compose run --rm pipeline --filter-uf SP --filter-cnae 62
```

## Filtragem de Dados

Processe apenas os dados que vocÃª precisa com filtros via linha de comando:

```bash
# Filtrar por estado (UF)
python main.py --filter-uf SP
python main.py --filter-uf SP,RJ,MG

# Filtrar por atividade econÃ´mica (CNAE)
python main.py --filter-cnae 62
python main.py --filter-cnae 62,47

# Filtrar por porte da empresa
python main.py --filter-porte 1,3  # ME e EPP

# Combinar filtros
python main.py --filter-uf SP --filter-cnae 62 --filter-porte 1

# Listar filtros disponÃ­veis
python main.py --list-filters
```

### Filtros DisponÃ­veis

| Filtro | DescriÃ§Ã£o | Exemplo |
|--------|-----------|---------|
| `--filter-uf` | Estados brasileiros | `SP,RJ,MG` |
| `--filter-cnae` | CÃ³digos de atividade (prefixo) | `62,47` |
| `--filter-porte` | Porte: 1=ME, 3=EPP, 5=Demais | `1,3` |

## ConfiguraÃ§Ã£o

### SeleÃ§Ã£o de Backend

```bash
# PostgreSQL (padrÃ£o e recomendado)
DATABASE_BACKEND=postgresql

# Suporte futuro
# DATABASE_BACKEND=mysql
# DATABASE_BACKEND=bigquery
# DATABASE_BACKEND=sqlite
```

### EstratÃ©gias de Processamento

O sistema detecta automaticamente a estratÃ©gia ideal:

| MemÃ³ria | EstratÃ©gia | DescriÃ§Ã£o |
|---------|------------|-----------|
| <8GB | `memory_constrained` | Processamento em chunks pequenos |
| 8-32GB | `high_memory` | Batches maiores, cache otimizado |
| >32GB | `distributed` | Processamento paralelo mÃ¡ximo |

### VariÃ¡veis de ConfiguraÃ§Ã£o

| VariÃ¡vel | PadrÃ£o | DescriÃ§Ã£o |
|----------|---------|-----------|
| `BATCH_SIZE` | `50000` | Tamanho do lote para operaÃ§Ãµes |
| `MAX_MEMORY_PERCENT` | `80` | Uso mÃ¡ximo de memÃ³ria |
| `TEMP_DIR` | `./temp` | DiretÃ³rio temporÃ¡rio |
| `DB_HOST` | `localhost` | Host PostgreSQL |
| `DB_PORT` | `5432` | Porta PostgreSQL |
| `DB_NAME` | `cnpj` | Nome do banco |

### OtimizaÃ§Ã£o de Performance

| VariÃ¡vel | PadrÃ£o | DescriÃ§Ã£o |
|----------|---------|-----------|
| `DOWNLOAD_STRATEGY` | `sequential` | `sequential` ou `parallel` |
| `DOWNLOAD_WORKERS` | `4` | NÃºmero de downloads paralelos |
| `KEEP_DOWNLOADED_FILES` | `false` | Manter arquivos para re-execuÃ§Ãµes |

## Deployment

Este Ã© um job batch que processa dados CNPJ e finaliza. A Receita Federal atualiza os dados mensalmente, entÃ£o agende a execuÃ§Ã£o mensal.

### ExecuÃ§Ã£o Manual

```bash
# Executar uma vez
docker-compose up

# Com downloads paralelos
DOWNLOAD_STRATEGY=parallel DOWNLOAD_WORKERS=3 docker-compose up

# Manter arquivos para re-execuÃ§Ãµes (economiza bandwidth)
KEEP_DOWNLOADED_FILES=true docker-compose up

# Ou sem Docker
python main.py

# Com filtros
python main.py --filter-uf SP --filter-cnae 62
```

### ExecuÃ§Ã£o Agendada (Mensal)

**Linux/Mac (cron):**
```bash
# Executar no dia 5 de cada mÃªs Ã s 2h da manhÃ£
crontab -e
# Adicionar:
0 2 5 * * cd /caminho/para/cnpj-data-pipeline && docker-compose up >> /var/log/cnpj-pipeline.log 2>&1
```

**Windows (Task Scheduler):**
- Criar tarefa agendada mensal
- Comando: `docker-compose up`

**Kubernetes:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cnpj-pipeline
spec:
  schedule: "0 2 5 * *"  # Dia 5 Ã s 2h
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cnpj-pipeline
            image: sua-imagem
          restartPolicy: OnFailure
```

**GitHub Actions:**
```yaml
on:
  schedule:
    - cron: '0 2 5 * *'  # Dia 5 Ã s 2h UTC
```

### Plataformas que Requerem Containers Ativos

Algumas plataformas (PaaS) esperam que containers permaneÃ§am em execuÃ§Ã£o. Se necessÃ¡rio:

```bash
# Manter container ativo
docker run -d --name cnpj sua-imagem tail -f /dev/null

# Agendar execuÃ§Ã£o mensal do comando:
docker exec cnpj python main.py
```

## Arquitetura

```
cnpj-data-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py          # ConfiguraÃ§Ã£o com auto-detecÃ§Ã£o
â”‚   â”œâ”€â”€ downloader.py      # Download e extraÃ§Ã£o
â”‚   â”œâ”€â”€ processor.py       # Parsing e transformaÃ§Ã£o
â”‚   â”œâ”€â”€ filters/           # Sistema de filtros
â”‚   â”‚   â”œâ”€â”€ base.py        # Interface de filtros
â”‚   â”‚   â”œâ”€â”€ location.py    # Filtros geogrÃ¡ficos
â”‚   â”‚   â”œâ”€â”€ business.py    # Filtros de negÃ³cio
â”‚   â”‚   â””â”€â”€ registry.py    # Factory de filtros
â”‚   â”œâ”€â”€ download_strategies/ # EstratÃ©gias de download
â”‚   â”‚   â”œâ”€â”€ sequential.py  # Download sequencial
â”‚   â”‚   â””â”€â”€ parallel.py    # Download paralelo
â”‚   â””â”€â”€ database/          # AbstraÃ§Ã£o de banco de dados
â”‚       â”œâ”€â”€ base.py        # Interface abstrata
â”‚       â”œâ”€â”€ factory.py     # Factory pattern
â”‚       â””â”€â”€ postgres.py    # ImplementaÃ§Ã£o PostgreSQL
â”œâ”€â”€ main.py                # Ponto de entrada
â””â”€â”€ setup.py               # Assistente de configuraÃ§Ã£o
```

## Fluxo de Processamento

1. **Descoberta**: Localiza diretÃ³rio mais recente de dados CNPJ
2. **Download**: Baixa e extrai arquivos ZIP com retry automÃ¡tico (paralelo opcional)
3. **Filtragem**: Aplica filtros selecionados para reduzir dados processados
4. **Processamento**: Parse dos CSVs com estratÃ©gia adaptativa
5. **Carga**: Bulk upsert otimizado no banco de dados
6. **Rastreamento**: Marca arquivos como processados

## Tipos de Arquivo Suportados

| Arquivo | Tabela | DescriÃ§Ã£o |
|---------|--------|-----------|
| `CNAECSV` | `cnaes` | ClassificaÃ§Ãµes de atividade econÃ´mica |
| `EMPRECSV` | `empresas` | Registros de empresas |
| `ESTABELECSV` | `estabelecimentos` | Dados de estabelecimentos |
| `MOTICSV` | `motivos_situacao_cadastral` | Motivos de situaÃ§Ã£o cadastral |
| `MUNICCSV` | `municipios` | CÃ³digos de municÃ­pios |
| `NATJUCSV` | `naturezas_juridicas` | Naturezas jurÃ­dicas |
| `PAISCSV` | `paises` | CÃ³digos de paÃ­ses |
| `QUALSCSV` | `qualificacoes_socios` | QualificaÃ§Ãµes de sÃ³cios |
| `SIMPLECSV` | `dados_simples` | Dados do Simples Nacional |
| `SOCIOCSV` | `socios` | Quadro societÃ¡rio |

## Performance

Tempos tÃ­picos de processamento:

| Sistema | MemÃ³ria | Tempo (60M+ empresas) |
|---------|---------|---------------------|
| VPS bÃ¡sico | 4GB | ~8 horas |
| Servidor padrÃ£o | 16GB | ~2 horas |
| Servidor high-end | 64GB+ | ~1 hora |

## Desenvolvimento

### PrincÃ­pios de Design

- **Modular**: Cada componente com responsabilidade Ãºnica
- **Resiliente**: Tratamento de erros e retry automÃ¡tico
- **Eficiente**: Uso otimizado de memÃ³ria e operaÃ§Ãµes bulk
- **Adaptativo**: Ajuste automÃ¡tico aos recursos disponÃ­veis

### Adicionando Novo Backend

1. Criar adapter em `src/database/seu_banco.py`
2. Implementar mÃ©todos abstratos de `DatabaseAdapter`
3. Registrar no factory em `src/database/factory.py`
4. Criar arquivo de requirements em `requirements/seu_banco.txt`

---

# ðŸ‡§ðŸ‡· CNPJ Data Pipeline (English)

A configurable, modular data pipeline for Brazilian CNPJ registry files. Smart processing of 50+ million companies with multi-database support.

## Key Features

- **Modular Architecture**: Clean separation with database abstraction
- **Multi-Database**: Full PostgreSQL support, placeholders for others
- **Smart Processing**: Auto-adapts to available resources
- **Advanced Filtering**: Filter by state, CNAE, and company size via CLI
- **Parallel Downloads**: Configurable strategy for optimized download speed
- **Incremental**: Tracks processed files
- **Optimized**: Efficient bulk operations
- **Easy Config**: Interactive setup + env vars

## Quick Start

### Interactive Setup

```bash
python setup.py
```

### Manual Setup

```bash
pip install -r requirements.txt
cp env.example .env
python main.py
```

### Docker

```bash
docker-compose --profile postgres up --build

# With data filtering
docker-compose run --rm pipeline --filter-uf SP --filter-cnae 62

# With parallel downloads
DOWNLOAD_STRATEGY=parallel DOWNLOAD_WORKERS=3 docker-compose up
```

## Data Filtering

Process only the data you need using command-line filters:

```bash
# Filter by state
python main.py --filter-uf SP,RJ

# Filter by economic activity (CNAE code prefix)
python main.py --filter-cnae 62,47

# Filter by company size (1=ME, 3=EPP, 5=Others)
python main.py --filter-porte 1,3

# Combine filters
python main.py --filter-uf SP --filter-cnae 62 --filter-porte 1

# List available filters
python main.py --list-filters
```

## Deployment

This is a batch job that processes CNPJ data and exits. Schedule it to run monthly.

### Manual Execution

```bash
# Run once
docker-compose up
```

### Scheduled Execution (Monthly)

**Linux/Mac (cron):**
```bash
# Run on the 5th of each month at 2 AM
0 2 5 * * cd /path/to/cnpj-pipeline && docker-compose up
```

**Other platforms:** Use your platform's scheduler (Task Scheduler, Kubernetes CronJob, GitHub Actions, etc.)

### Note for PaaS Platforms

If your platform requires containers to stay running:

```bash
# Keep container alive
docker run -d --name cnpj your-image tail -f /dev/null

# Schedule this command monthly:
docker exec cnpj python main.py
```

## Configuration

Set `DATABASE_BACKEND`, `PROCESSING_STRATEGY`, and optimization options in `.env` file:

```bash
# Performance optimizations
DOWNLOAD_STRATEGY=parallel    # sequential|parallel
DOWNLOAD_WORKERS=4           # Number of parallel downloads
KEEP_DOWNLOADED_FILES=false  # Keep files for re-runs (saves bandwidth)
```

## Architecture

Factory pattern for database adapters, intelligent resource detection, chunked processing for large files.

## Performance

Processes 60M+ records in 1-12 hours depending on system resources.

Made with engineering excellence for the Brazilian tech community.
