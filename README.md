# üáßüá∑ CNPJ Data Pipeline

Um script modular e configur√°vel para processar arquivos CNPJ da Receita Federal do Brasil. Processamento inteligente de 50+ milh√µes de empresas com suporte a m√∫ltiplos bancos de dados.

## Caracter√≠sticas Principais

- **Arquitetura Modular**: Separa√ß√£o clara de responsabilidades com camada de abstra√ß√£o de banco de dados
- **Multi-Banco**: PostgreSQL totalmente suportado, com placeholders para MySQL, BigQuery e SQLite
- **Processamento Inteligente**: Adapta√ß√£o autom√°tica da estrat√©gia baseada em recursos dispon√≠veis
- **Processamento Incremental**: Rastreamento de arquivos processados para evitar duplica√ß√µes
- **Performance Otimizada**: Opera√ß√µes bulk eficientes com tratamento de conflitos
- **Configura√ß√£o Simples**: Setup interativo + vari√°veis de ambiente

## In√≠cio R√°pido

### Op√ß√£o 1: Setup Interativo (Recomendado)

```bash
# Clone o reposit√≥rio
git clone https://github.com/cnpj-chat/cnpj-data-pipeline
cd cnpj-data-pipeline

# Execute o assistente de configura√ß√£o
python setup.py
```

O assistente ir√°:
- Detectar recursos do sistema
- Configurar conex√£o com banco de dados
- Instalar depend√™ncias necess√°rias
- Criar configura√ß√£o otimizada

### Op√ß√£o 2: Configura√ß√£o Manual

```bash
# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar ambiente
cp env.example .env
# Editar .env com suas configura√ß√µes

# Executar
python main.py
```

### Docker

```bash
# PostgreSQL (padr√£o)
docker-compose --profile postgres up --build

# Com configura√ß√µes customizadas
DATABASE_BACKEND=postgresql BATCH_SIZE=100000 docker-compose --profile postgres up
```

## Configura√ß√£o

### Sele√ß√£o de Backend

```bash
# PostgreSQL (padr√£o e recomendado)
DATABASE_BACKEND=postgresql

# Suporte futuro
# DATABASE_BACKEND=mysql
# DATABASE_BACKEND=bigquery
# DATABASE_BACKEND=sqlite
```

### Estrat√©gias de Processamento

O sistema detecta automaticamente a estrat√©gia ideal:

| Mem√≥ria | Estrat√©gia | Descri√ß√£o |
|---------|------------|-----------|
| <8GB | `memory_constrained` | Processamento em chunks pequenos |
| 8-32GB | `high_memory` | Batches maiores, cache otimizado |
| >32GB | `distributed` | Processamento paralelo m√°ximo |

### Vari√°veis de Configura√ß√£o

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|---------|-----------|
| `BATCH_SIZE` | `50000` | Tamanho do lote para opera√ß√µes |
| `MAX_MEMORY_PERCENT` | `80` | Uso m√°ximo de mem√≥ria |
| `TEMP_DIR` | `./temp` | Diret√≥rio tempor√°rio |
| `DB_HOST` | `localhost` | Host PostgreSQL |
| `DB_PORT` | `5432` | Porta PostgreSQL |
| `DB_NAME` | `cnpj` | Nome do banco |

## Deployment

Este √© um job batch que processa dados CNPJ e finaliza. A Receita Federal atualiza os dados mensalmente, ent√£o agende a execu√ß√£o mensal.

### Execu√ß√£o Manual

```bash
# Executar uma vez
docker-compose up

# Ou sem Docker
python main.py
```

### Execu√ß√£o Agendada (Mensal)

**Linux/Mac (cron):**
```bash
# Executar no dia 5 de cada m√™s √†s 2h da manh√£
crontab -e
# Adicionar:
0 2 5 * * cd /caminho/para/cnpj-data-pipeline && docker-compose up >> /var/log/cnpj-pipeline.log 2>&1
```

**Windows (Task Scheduler):**
- Criar tarefa agendada mensal
- Comando: `docker-compose up`

**Kubernetes:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cnpj-pipeline
spec:
  schedule: "0 2 5 * *"  # Dia 5 √†s 2h
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cnpj-pipeline
            image: sua-imagem
          restartPolicy: OnFailure
```

**GitHub Actions:**
```yaml
on:
  schedule:
    - cron: '0 2 5 * *'  # Dia 5 √†s 2h UTC
```

### Plataformas que Requerem Containers Ativos

Algumas plataformas (PaaS) esperam que containers permane√ßam em execu√ß√£o. Se necess√°rio:

```bash
# Manter container ativo
docker run -d --name cnpj sua-imagem tail -f /dev/null

# Agendar execu√ß√£o mensal do comando:
docker exec cnpj python main.py
```

## Arquitetura

```
cnpj-data-pipeline/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configura√ß√£o com auto-detec√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ downloader.py      # Download e extra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ processor.py       # Parsing e transforma√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ database/          # Abstra√ß√£o de banco de dados
‚îÇ       ‚îú‚îÄ‚îÄ base.py        # Interface abstrata
‚îÇ       ‚îú‚îÄ‚îÄ factory.py     # Factory pattern
‚îÇ       ‚îî‚îÄ‚îÄ postgres.py    # Implementa√ß√£o PostgreSQL
‚îú‚îÄ‚îÄ main.py                # Ponto de entrada
‚îî‚îÄ‚îÄ setup.py               # Assistente de configura√ß√£o
```

## Fluxo de Processamento

1. **Descoberta**: Localiza diret√≥rio mais recente de dados CNPJ
2. **Download**: Baixa e extrai arquivos ZIP com retry autom√°tico
3. **Processamento**: Parse dos CSVs com estrat√©gia adaptativa
4. **Carga**: Bulk upsert otimizado no banco de dados
5. **Rastreamento**: Marca arquivos como processados

## Tipos de Arquivo Suportados

| Arquivo | Tabela | Descri√ß√£o |
|---------|--------|-----------|
| `CNAECSV` | `cnaes` | Classifica√ß√µes de atividade econ√¥mica |
| `EMPRECSV` | `empresas` | Registros de empresas |
| `ESTABELECSV` | `estabelecimentos` | Dados de estabelecimentos |
| `MOTICSV` | `motivos_situacao_cadastral` | Motivos de situa√ß√£o cadastral |
| `MUNICCSV` | `municipios` | C√≥digos de munic√≠pios |
| `NATJUCSV` | `naturezas_juridicas` | Naturezas jur√≠dicas |
| `PAISCSV` | `paises` | C√≥digos de pa√≠ses |
| `QUALSCSV` | `qualificacoes_socios` | Qualifica√ß√µes de s√≥cios |
| `SIMPLECSV` | `dados_simples` | Dados do Simples Nacional |
| `SOCIOCSV` | `socios` | Quadro societ√°rio |

## Performance

Tempos t√≠picos de processamento:

| Sistema | Mem√≥ria | Tempo (60M+ empresas) |
|---------|---------|---------------------|
| VPS b√°sico | 4GB | ~8 horas |
| Servidor padr√£o | 16GB | ~2 horas |
| Servidor high-end | 64GB+ | ~1 hora |

## Desenvolvimento

### Princ√≠pios de Design

- **Modular**: Cada componente com responsabilidade √∫nica
- **Resiliente**: Tratamento de erros e retry autom√°tico
- **Eficiente**: Uso otimizado de mem√≥ria e opera√ß√µes bulk
- **Adaptativo**: Ajuste autom√°tico aos recursos dispon√≠veis

### Adicionando Novo Backend

1. Criar adapter em `src/database/seu_banco.py`
2. Implementar m√©todos abstratos de `DatabaseAdapter`
3. Registrar no factory em `src/database/factory.py`
4. Criar arquivo de requirements em `requirements/seu_banco.txt`

---

# üáßüá∑ CNPJ Data Pipeline (English)

A configurable, modular data pipeline for Brazilian CNPJ registry files. Smart processing of 50+ million companies with multi-database support.

## Key Features

- **Modular Architecture**: Clean separation with database abstraction
- **Multi-Database**: Full PostgreSQL support, placeholders for others
- **Smart Processing**: Auto-adapts to available resources
- **Incremental**: Tracks processed files
- **Optimized**: Efficient bulk operations
- **Easy Config**: Interactive setup + env vars

## Quick Start

### Interactive Setup

```bash
python setup.py
```

### Manual Setup

```bash
pip install -r requirements.txt
cp env.example .env
python main.py
```

### Docker

```bash
docker-compose --profile postgres up --build
```

## Deployment

This is a batch job that processes CNPJ data and exits. Schedule it to run monthly.

### Manual Execution

```bash
# Run once
docker-compose up
```

### Scheduled Execution (Monthly)

**Linux/Mac (cron):**
```bash
# Run on the 5th of each month at 2 AM
0 2 5 * * cd /path/to/cnpj-pipeline && docker-compose up
```

**Other platforms:** Use your platform's scheduler (Task Scheduler, Kubernetes CronJob, GitHub Actions, etc.)

### Note for PaaS Platforms

If your platform requires containers to stay running:

```bash
# Keep container alive
docker run -d --name cnpj your-image tail -f /dev/null

# Schedule this command monthly:
docker exec cnpj python main.py
```

## Configuration

Set `DATABASE_BACKEND` and `PROCESSING_STRATEGY` in `.env` file.

## Architecture

Factory pattern for database adapters, intelligent resource detection, chunked processing for large files.

## Performance

Processes 60M+ records in 1-12 hours depending on system resources.

Made with engineering excellence for the Brazilian tech community.
